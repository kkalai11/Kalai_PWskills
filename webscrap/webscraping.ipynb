{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "eb scraping is the process of automatically extracting data from websites. It involves fetching the web pages and parsing the content to retrieve the desired information. Web scraping is used to gather large amounts of data from the web efficiently and quickly, which can then be analyzed or used for various purposes.\n",
    "\n",
    "Three areas where web scraping is used to get data are:\n",
    "1. **E-commerce**: To monitor competitor prices, product availability, and customer reviews.\n",
    "2. **Research**: To collect data from academic publications, forums, and social media for analysis.\n",
    "3. **Real Estate**: To gather information on property listings, prices, and trends from various real estate websites.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What are the different types of web scrapping ?\n",
    "Web scraping can be performed through various methods depending on the nature of the website and data being extracted. Here's a brief summary:\n",
    "\n",
    "1. **Manual Web Scraping**: Copying and pasting data manually; suitable for small-scale extraction but time-consuming and inefficient.\n",
    "2. **Web Scraping Libraries/Frameworks**: Using tools like BeautifulSoup, Scrapy, and lxml for structured data extraction from static HTML.\n",
    "3. **Browser Automation**: Tools like Selenium and Puppeteer automate browsers to handle dynamic, JavaScript-heavy websites.\n",
    "4. **API Scraping**: Extracting data via official APIs for structured, efficient access, avoiding the need for scraping HTML.\n",
    "5. **Headless Browsing**: Using browsers in headless mode (without a GUI) for faster scraping of dynamic content.\n",
    "6. **XPath Scraping**: Locating elements in HTML with XPath for precise data extraction from nested structures.\n",
    "7. **Regular Expressions (Regex)**: Searching for patterns in raw HTML/text to extract specific data.\n",
    "8. **Web Scraping via CMS**: Easier scraping from websites built with common Content Management Systems like WordPress.\n",
    "9. **Proxy Rotation and CAPTCHA Solving**: Bypassing scraping blocks with proxies and CAPTCHA-solving services.\n",
    "10. **Web Crawling**: Systematically scraping multiple pages or entire websites, often combined with scraping.\n",
    "11. **PDF Data Extraction**: Extracting data from PDF files using tools like PyPDF2 and Tabula.\n",
    "\n",
    "The choice of method depends on the complexity of the website, the need for interaction, and the volume of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q3. What is Beautiful Soup? Why is it used?\n",
    "Beautiful Soup is a Python library used for web scraping to extract data from HTML or XML documents. It simplifies parsing, navigating, and searching the structure of web pages, allowing easy extraction of data such as headlines, links, and tables. It handles malformed HTML and provides clean, well-structured output. Beautiful Soup is ideal for scraping static websites with well-formed HTML and is often used in combination with libraries like requests or Selenium. It's best for extracting specific data from simple, static web pages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Why is flask used in this Web Scraping project?\n",
    "Flask is used in web scraping projects to create simple web interfaces or APIs to display and interact with scraped data. It allows integration with scraping libraries like Beautiful Soup, handles automation tasks, and provides a lightweight framework for building user-friendly applications. Flask is ideal for creating dynamic interfaces where users can trigger or view the results of web scraping in real-time. Its flexibility makes it suitable for small-to-medium-scale projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "In a web scraping project, AWS services help manage infrastructure, storage, automation, and scaling. EC2 or Lambda can run scraping tasks, while S3 and RDS store the data. Services like CloudWatch and Step Functions monitor and orchestrate tasks, and API Gateway can expose scraped data via APIs. SQS and SNS help manage queues and notifications, ensuring smooth operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
